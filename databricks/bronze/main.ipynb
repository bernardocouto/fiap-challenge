{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXIST bronze\n",
    "COMMENT 'Bronze Layer'\n",
    "LOCALTION 's3://fiap-challenge/databricks/bronze';\n",
    "\n",
    "CREATE TABLE bronze.health (\n",
    "\n",
    ");\n",
    "\n",
    "CREATE TABLE bronze.sales (\n",
    "\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "checkpoint_location_health = 's3://fiap-challenge/databricks/checkpoint/bronze/health'\n",
    "checkpoint_location_sales = 's3://fiap-challenge/databricks/checkpoint/bronze/sales'\n",
    "\n",
    "path_health = 's3://fiap-challenge/databricks/landing/health'\n",
    "path_sales = 's3://fiap-challenge/databricks/landing/sales'\n",
    "\n",
    "read_options_health = {'sep': ';', 'header': False}\n",
    "read_options_sales = {'sep': ';', 'header': False}\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config('spark.hadoop.hive.exec.dynamic.partition', 'true') \\\n",
    "    .config('spark.hadoop.hive.exec.dynamic.partition.mode', 'nonstrict') \\\n",
    "    .config('spark.sql.sources.partitionOverwriteMode', 'dynamic') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .appName('fiap-challenge') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "df_health = spark \\\n",
    "    .readStream \\\n",
    "    .format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'csv') \\\n",
    "    .option(**read_options_health) \\\n",
    "    .load(path_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "df_sales = spark \\\n",
    "    .readStream \\\n",
    "    .format('cloudFiles') \\\n",
    "    .option('cloudFiles.format', 'csv') \\\n",
    "    .option(**read_options_sales) \\\n",
    "    .load(path_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "stream_health = df_health \\\n",
    "    .writeStream \\\n",
    "    .format('delta') \\\n",
    "    .option('checkpointLocation', checkpoint_location_health) \\\n",
    "    .trigger(once=True)\n",
    "\n",
    "stream_health \\\n",
    "    .toTable(tableName='bronze.health')\n",
    "\n",
    "stream_health.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "stream_sales = df_sales \\\n",
    "    .writeStream \\\n",
    "    .format('delta') \\\n",
    "    .option('checkpointLocation', checkpoint_location_health) \\\n",
    "    .trigger(once=True)\n",
    "\n",
    "stream_sales \\\n",
    "    .toTable(tableName='bronze.sales')\n",
    "\n",
    "stream_sales.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
